# -*- coding: utf-8 -*-
"""LSTM & GRU_CIPLA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HFUPLarXZkilZJDGqwHQG0SBWWstvxcr
"""

from google.colab import files
uploaded = files.upload()

import matplotlib.pyplot as plt
import pandas as pd
import datetime as dt
import numpy as np
from sklearn.preprocessing import StandardScaler

# Import data 
df = pd.read_csv("CIPLA.NS.csv", parse_dates= True)
df

print(df.info())
print('\n')
print(df.isnull().sum())

df = df.fillna(method ='pad') # filling the missing values with previous ones 
print (df.isnull().sum())
print('\n')
print('There are {} number of days in the dataset.'.format(df.shape[0]))

import seaborn as sns
plt.figure(figsize=(10,6))

dataset = df[['Open', 'High', 'Low', 'Close']] # dataframe with open, high, low, close

print("\n")
f, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True)
sns.distplot( dataset["Open"] , ax=axes[0, 0])
sns.distplot( dataset["High"] , ax=axes[0, 1])
sns.distplot(dataset["Low"] , ax=axes[1, 0])
sns.distplot( dataset["Close"] , ax=axes[1, 1])
plt.show()
print('\n')
dataset.describe()

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

plt.figure(figsize=(10,6))
plt.plot(df["Open"])
plt.plot(df["High"])
plt.plot(df["Low"])
plt.plot(df["Close"])
plt.title('CIPLA price history')
plt.ylabel('Price (INR)')
plt.legend(['Open','High','Low','Close'], loc='best')
plt.show()

import copy
data = dataset.copy()
tek_ind_1 = copy.deepcopy(data)
tek_ind_2 = copy.deepcopy(data)

# Commented out IPython magic to ensure Python compatibility.
!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzvf ta-lib-0.4.0-src.tar.gz
# %cd ta-lib
!./configure --prefix=/usr
!make
!make install
!pip install Ta-Lib
import talib

from talib import RSI, BBANDS

tek_ind_1['daily_return'] = tek_ind_1.Close.pct_change().fillna(0)
tek_ind_1['cum_daily_return'] = (1 + tek_ind_1['daily_return']).cumprod()

tek_ind_1['H-L'] = tek_ind_1.High - dataset.Low

tek_ind_1 ['C-O'] = tek_ind_1.Close - tek_ind_1.Open

tek_ind_1['10day MA'] = tek_ind_1.Close.shift(1).rolling(window = 10).mean().fillna(0)
tek_ind_1['50day MA'] = tek_ind_1.Close.shift(1).rolling(window = 50).mean().fillna(0)
tek_ind_1['200day MA'] = tek_ind_1.Close.shift(1).rolling(window = 200).mean().fillna(0)

tek_ind_1['rsi'] = talib.RSI(tek_ind_1.Close.values, timeperiod = 14)

tek_ind_1['Williams %R'] = talib.WILLR(tek_ind_1.High.values, 
                                     tek_ind_1.Low.values, 
                                     tek_ind_1.Close.values, 14)

# Create 7 and 21 days Moving Average
tek_ind_1['ma7'] = tek_ind_1.Close.rolling(window=7).mean().fillna(0)
tek_ind_1['ma21'] = tek_ind_1.Close.rolling(window=21).mean().fillna(0)
    
# Creating MACD
tek_ind_1['ema_26'] = tek_ind_1.Close.ewm(span=26).mean().fillna(0)
tek_ind_1['ema_12'] = tek_ind_1.Close.ewm(span=12).mean().fillna(0)
tek_ind_1['macd'] = (tek_ind_1['ema_12'] - tek_ind_1['ema_26'])

# Creating Bollinger Bands
#Set number of days and standard deviations to use for rolling lookback period for Bollinger band calculation
window = 21
no_of_std = 2
#Calculate rolling mean and standard deviation using number of days set above
rolling_mean = tek_ind_1.Close.rolling(window).mean()
rolling_std = tek_ind_1.Close.rolling(window).std()
#create two new DataFrame columns to hold values of upper and lower Bollinger bands
#B['Rolling Mean'] = rolling_mean.fillna(0)
tek_ind_1['bb_high'] = (rolling_mean + (rolling_std * no_of_std)).fillna(0)
tek_ind_1['bb_low'] = (rolling_mean - (rolling_std * no_of_std)).fillna(0)
    
# Create Exponential moving average
tek_ind_1['ema'] = tek_ind_1.Close.ewm(com=0.5).mean()
    
# Create Momentum
tek_ind_1['momentum'] = tek_ind_1.Close - 1

tek_ind_1.head(20)

# Plot the cumulative daily returns
tek_ind_1['cum_daily_return'].plot(figsize=(12,8))
plt.title('Cumulative daily return')

# Show the plot
plt.show()

tek_ind_1['daily_return'].describe()

plt.figure(figsize=(12, 8))
plt.plot(tek_ind_1['Close'], label='Actual')
plt.plot(tek_ind_1['10day MA'], label='10day MA')
plt.plot(tek_ind_1['50day MA'], label='50day MA')
plt.plot(tek_ind_1['200day MA'], label='200day MA')

plt.legend(loc='best')
plt.show()
print(tek_ind_1.info())

# Plot first subplot
plt.figure(figsize=(15,8))
plt.plot(tek_ind_1['ma7'],label='MA 7', color='g',linestyle='--')
plt.plot(tek_ind_1['Close'],label='Closing Price', color='b')
plt.plot(tek_ind_1['ma21'],label='MA 21', color='r',linestyle='--')
plt.plot(tek_ind_1['bb_high'],label='Upper Band', color='c')
plt.plot(tek_ind_1['bb_low'],label='Lower Band', color='c')
plt.title('Technical indicators for CIPLA')
plt.ylabel('INR')
plt.legend()

"""##tek_ind_2 **dataframe**"""

def stok(df, n):
    tek_ind_2['stok'] = ((tek_ind_2['Close'] - tek_ind_2['Low'].rolling(window=n, center=False).mean()) / 
                  (tek_ind_2['High'].rolling(window=n, center=False).max() - 
                   tek_ind_2['Low'].rolling(window=n, center=False).min())) * 100
    tek_ind_2['stok'] = tek_ind_2['stok'].rolling(window = 3, center=False).mean()
    
stok(tek_ind_2, 4)
tek_ind_2 = tek_ind_2.fillna(0)
tek_ind_2.tail()

"""### CCI = (typical price − ma) / (0.015 * mean deviation)
- typical price = (high + low + close) / 3
- p = number of periods (20 commonly used)
- ma = moving average
- moving average = typical price / p
- mean deviation = (typical price - MA) / p
### Calculations of Ichimoku Cloud
- Turning Line = ( Highest High + Lowest Low ) / 2, for the past 9 days
- Standard Line = ( Highest High + Lowest Low ) / 2, for the past 26 days
- Leading Span 1 = ( Standard Line + Turning Line ) / 2, plotted 26 days ahead of today
- Leading Span 2 = ( Highest High + Lowest Low ) / 2, for the past 52 days, plotted 26 days ahead of today
- Cloud = Shaded Area between Span 1 and Span 2
"""

#Calculation of Price Rate of Change
# ROC = [(Close - Close n periods ago) / (Close n periods ago)] * 100

tek_ind_2['ROC'] = ((tek_ind_2['Close'] - tek_ind_2['Close'].shift(12)) / 
                    (tek_ind_2['Close'].shift(12)))*100
tek_ind_2 = tek_ind_2.fillna(0)

#Calculation of Momentum
tek_ind_2['Momentum'] = tek_ind_2['Close'] - tek_ind_2['Close'].shift(4)
tek_ind_2 = tek_ind_2.fillna(0)

#Calculation of Commodity Channel Index
tp = (tek_ind_2['High'] + tek_ind_2['Low'] + tek_ind_2['Close']) / 3
ma = tp / 20
md = (tp - ma) / 20
tek_ind_2['CCI'] = (tp-ma)/(0.015 * md)

# Calculation of Triple Exponential Moving Average
# Triple Exponential MA Formula:
# T-EMA = (3EMA – 3EMA(EMA)) + EMA(EMA(EMA))
# Where:
# EMA = EMA(1) + α * (Close – EMA(1))
# α = 2 / (N + 1)
# N = The smoothing period.

tek_ind_2['ema'] = tek_ind_2['Close'].ewm(span=3,min_periods=0,adjust=True,ignore_na=False).mean()
tek_ind_2 = tek_ind_2.fillna(0)

tek_ind_2['tema'] = (3 * tek_ind_2['ema'] - 3 * tek_ind_2['ema'] * tek_ind_2['ema']) + (tek_ind_2['ema'] * 
                                                                                        tek_ind_2['ema'] * 
                                                                                        tek_ind_2['ema'])


# Turning Line
high = tek_ind_2['High'].rolling(window=9,center=False).max()
low = tek_ind_2['Low'].rolling(window=9,center=False).min()
tek_ind_2['turning_line'] = (high + low) / 2

# Standard Line
p26_high = tek_ind_2['High'].rolling(window=26,center=False).max()
p26_low = tek_ind_2['Low'].rolling(window=26,center=False).min()
tek_ind_2['standard_line'] = (p26_high + p26_low) / 2

# Leading Span 1
tek_ind_2['ichimoku_span1'] = ((tek_ind_2['turning_line'] + tek_ind_2['standard_line']) / 2).shift(26)

# Leading Span 1
tek_ind_2['ichimoku_span1'] = ((tek_ind_2['turning_line'] + tek_ind_2['standard_line']) / 2).shift(26)
    
# Leading Span 2
p52_high = tek_ind_2['High'].rolling(window=52,center=False).max()
p52_low = tek_ind_2['Low'].rolling(window=52,center=False).min()
tek_ind_2['ichimoku_span2'] = ((p52_high + p52_low) / 2).shift(26)
    
# The most current closing price plotted 22 time periods behind (optional)
tek_ind_2['chikou_span'] = tek_ind_2['Close'].shift(-22) # 22 according to investopedia

"""### Fourier transformation"""

close_fft = np.fft.fft(np.asarray(dataset.Close.tolist()))
fft_df = pd.DataFrame({'fft':close_fft})
fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))
fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))

plt.figure(figsize=(14, 7), dpi=100)
fft_list = np.asarray(fft_df['fft'].tolist())
for num_ in [3, 6, 9, 100]:
    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0
    plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))
plt.plot(tek_ind_2.Close,  label='Real')
plt.xlabel('Days')
plt.ylabel('INR')
plt.title('CIPLA (close) stock prices & Fourier transforms')
plt.legend()
plt.show()

tek_ind_2['absolute'] = fft_df['absolute']
tek_ind_2['angle'] = fft_df['angle']
print(tek_ind_2.head().fillna(0))
print('\n')
print(tek_ind_2.info())

"""## **LSTM Model for Prediction of movement of stocks**"""

print(tek_ind_1.columns)

a = copy.deepcopy(tek_ind_1)
#b = copy.deepcopy(tek_ind_2)
print(a.head())
print('\n')
print('Total dataset has {} samples, and {} features.'.format(a.shape[0], a.shape[1]))
print('\n')
a.info()

a = a.fillna(0) # removing NaN from columns
a.info()

from math import sqrt
from numpy import split, array
from sklearn.metrics import mean_squared_error

a.index

values = a.values
# ensure all data is float
values = values.astype('float32')
values

print("Min:", np.min(values))
print("Max:", np.max(values))

values =pd.DataFrame(values)
values.head(4)

def ts (a, look_back = 60, pred_col = 4):
    t = a.copy()
    t["id"] = range(1, len(t)+1)
    t = t.iloc[:-look_back, :]
    t.set_index('id', inplace =True)
    pred_value = a.copy()
    pred_value = pred_value.iloc[look_back:, pred_col]
    pred_value.columns = ["Pred"]
    pred_value = pd.DataFrame(pred_value)
    
    pred_value["id"] = range(1, len(pred_value)+1)
    pred_value.set_index('id', inplace = True)
    final_df= pd.concat([t, pred_value], axis=1)
    
    return final_df

arr_df = ts(values, 60,4)
arr_df.fillna(0, inplace=True)

arr_df.columns = ['v1(t-60)', 'v2(t-60)', 'v3(t-60)', 'v4(t-60)', 
                       'v5(t-60)', 'v6(t-60)', 'v7(t-60)', 'v8(t-60)',
                       'v9(t-60)', 'v10(t-60)', 'v11(t-60)', 'v12(t-60)',
                       'v13(t-60)', 'v14(t-60)', 'v15(t-60)', 'v16(t-60)',
                       'v17(t-60)', 'v18(t-60)', 'v19(t-60)', 'v20(t-60)',
                       'v21(t-60)', 'v22(t-60)', 'v1(t)']
print(arr_df.head(4))

print(arr_df.describe())

arr_df.shape

# split into train and test sets
val = arr_df.values
train_sample =int(len(a)*0.8)
train = val[: train_sample, :]
test = val[train_sample:, :]

print(train.shape, test.shape)

train = pd.DataFrame(train)
test = pd.DataFrame(test)

X, y = train, test
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X= scaler.fit_transform(X)
print(X)
print('\n')
print(X.shape)

# shaping data from neural network
X_train = []
y_train = []
for i in range(60, X.shape[0]):
  X_train.append(X[i-60:i])
  y_train.append(X[i,0])
  if i <= 61:
    print(X_train)
    print('\n')
    print(y_train)
    print()

import numpy as np
X_train, y_train = np.array(X_train), np.array(y_train)
print(X_train.shape, y_train.shape)

from sklearn import metrics # for the check the error and accuracy of the model
from sklearn.metrics import mean_squared_error,r2_score

from __future__ import absolute_import, division, print_function, unicode_literals

try:
  import tensorflow.compat.v2 as tf
except Exception:
  pass

tf.enable_v2_behavior()

print(tf.__version__)

from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping

#create model
model_lstm = tf.keras.Sequential()
model_lstm.add(tf.keras.layers.LSTM(units = 75, return_sequences = True, 
                                   input_shape = (X_train.shape[1], X_train.shape[2])))
model_lstm.add(tf.keras.layers.LSTM(units = 30,return_sequences = True))
model_lstm.add(tf.keras.layers.LSTM(units = 30, return_sequences = True))
  
model_lstm.add(tf.keras.layers.Dense(units = 1))
model_lstm.compile(loss='mae', optimizer='adam')
model_lstm.summary()

test = pd.DataFrame(test)
test

look_back = train.tail(60)
data = look_back.append(test)
print(data)

inputs = scaler.transform(data)
inputs

# shaping data from neural network
X_test = []
y_test = []
for i in range(60, inputs.shape[0]):
  X_test.append(inputs[i-60:i])
  y_test.append(inputs[i,0])
  if i <= 61:
    print(X_test)
    print('\n')
    print(y_test)
    print()

X_test, y_test = np.array(X_test), np.array(y_test)
print(X_test.shape, y_test.shape)

# fit network
history_lstm = model_lstm.fit(X_train, y_train, 
                              epochs = 20, 
                              batch_size = 32, 
                              validation_data = (X_test, y_test),  
                              shuffle=False)

model_gru = tf.keras.Sequential()
model_gru.add(tf.keras.layers.GRU(75, return_sequences = True, input_shape=(X_train.shape[1], X_train.shape[2])))
model_gru.add(tf.keras.layers.GRU(units=30, return_sequences=True))
model_gru.add(tf.keras.layers.GRU(units=30))
model_gru.add(tf.keras.layers.Dense(units=1))

model_gru.compile(loss='mae', optimizer='adam')
model_gru.summary()

# fit network
gru_history = model_gru.fit(X_train, y_train, epochs = 20, batch_size = 64, 
                        validation_data = (X_test, y_test), shuffle=False)

plt.figure(figsize=(10, 6), dpi=100)
plt.plot(history_lstm.history['loss'], label='LSTM train', color='red')
plt.plot(history_lstm.history['val_loss'], label='LSTM test', color= 'green')
plt.plot(gru_history.history['loss'], label='GRU train', color='brown')
plt.plot(gru_history.history['val_loss'], label='GRU test', color='blue')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.title('Training and Validation loss')
plt.show()

y_pred = model_lstm.predict(X_test)

y_pred

scaler.scale_

normal_scale = 1/0.00143499
normal_scale

y_pred = y_pred * normal_scale
y_pred

y_test = y_test * normal_scale
y_test

mean_y_test = y_test.mean()
mean_y_pred = y_pred.mean()
print(mean_y_test, mean_y_pred)

accuracy = round((mean_y_test / mean_y_pred)*100,2)
accuracy

gru_pred = model_gru.predict(X_test)

scaler.scale_

scale = 1/0.00143499
scale

gru_pred = gru_pred * scale
gru_pred

gru_test = y_test * scale
gru_test

mean_gru_test = y_test.mean()
mean_y_pred = y_pred.mean()
print(mean_y_test, mean_y_pred)